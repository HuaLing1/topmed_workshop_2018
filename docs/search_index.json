[
["index.html", "TOPMed Analysis Workshop 1 Introduction", " TOPMed Analysis Workshop 2017-08-03 1 Introduction This site contains course materials for the TOPMed Analysis Workshop, August 7-9 2017. To work through the exercises, log into RStudio Server at http://52.25.72.68:8787 with your username and password. Slides for lectures will be posted here: https://www.nhlbiwgs.org/topmed-analysis-workshop-2017-materials Join the Slack channel here: https://join.slack.com/t/topmed-analysis-2017/signup If you are new to R, you might find the following material helpful: Introduction to R class from the Summer Institute in Statistical Genetics Graphics with ggplot2 tutorial Data manipulation with dplyr "],
["downloading-data-from-the-topmed-exchange-area.html", "2 Downloading Data From The TOPMed Exchange Area 2.1 What is the TOPMed Exchange Area (EA)? 2.2 How does the EA work? 2.3 What data can I find in the EA? 2.4 Who can access the TOPMed EA? 2.5 How can I download data from the EA? 2.6 Quick How-to 2.7 Useful resources", " 2 Downloading Data From The TOPMed Exchange Area If you know you have access to your study’s TOPMed Exchange Area, you can follow along with these instructions: https://www.nhlbiwgs.org/system/files/Information/How%20to%20Download%20Data%20from%20the%20dbGaP%20Exchange%20Area.pdf 2.1 What is the TOPMed Exchange Area (EA)? Exchange Areas are a special section of dbGaP storage that facilitates collaboration in large multicenter projects. The TOPMed EA allows pre-release data (genotypes, phenotypes, etc.) to be stored and exchanged between TOPMed investigators. 2.2 How does the EA work? There isn’t just one TOPMed EA, but one EA for each TOPMed project study Each project study’s TOPMed accession has a section of “Provisional files” - this is the EA You’ll get more information about dbGaP parent accession vs. child accession vs. TOPMed accession later Project studies may upload additional data to the EA, where those with access can download it TOPMed genotype freezes are available for TOPMed investigators before general release EA data should only be downloaded to appropriately protected computer systems (i.e. not your personal laptop) 2.3 What data can I find in the EA? IRC’s combined sequencing result .vcfs (genotype freezes) DCC-harmonized phenotypes DCC’s sample annotation Corrected sample swaps Unique individuals (excludes duplicates, MZ twins) Inferred sex chromosome karyotype DCC’s kinship estimates DCC’s principal components Phenotype data shared for specific working groups 2.4 Who can access the TOPMed EA? An “elligible investigator” who has been nominated for access by a TOPMed project study A designated downloader for one of the nominated investigators If you are an analyst who needs to work with TOPMed data, you will probably gain access to it through your PI See detailed information at the TOPMed website 2.5 How can I download data from the EA? An elligible investigator must be nominated by their study and complete a Data Access Request (DAR) before accessing the EA data See detailed instructions (with screenshots!) on the TOPMed website 2.6 Quick How-to Log in to the dbGaP controlled access area Go to “My Requests” tab Find the TOPMed accession (has “NHLBI TOPMed” in the study name) for a study you’re associated with and click on “Request Files” Click on the “Provisional files” tab Select the files you want to download Use the information provided to start an Aspera download of the data 2.7 Useful resources TOPMed Data Sharing Policies List of TOPMed study accession numbers General dbGaP download instructions "],
["gds-format.html", "3 GDS format 3.1 Exploring a GDS file 3.2 Exercise: HWE", " 3 GDS format GDS is Genomic Data Structure, a storage format that can efficiently store genomic data and provide fast random access to subsets of the data. For more information on GDS for sequence data, read the SeqArray package vignette. 3.1 Exploring a GDS file To use the R packages developed at the DCC for sequence data, we first need to convert a VCF file to GDS. (If the file is BCF, use https://samtools.github.io/bcftools/bcftools.html to convert to VCF.) library(SeqArray) data.path &lt;- &quot;https://github.com/smgogarten/analysis_pipeline/raw/devel/testdata&quot; vcffile &lt;- &quot;1KG_phase3_subset_chr1.vcf.gz&quot; if (!file.exists(vcffile)) download.file(file.path(data.path, vcffile), vcffile) gdsfile &lt;- &quot;1KG_phase3_subset_chr1.gds&quot; seqVCF2GDS(vcffile, gdsfile, fmt.import=&quot;GT&quot;, storage.option=&quot;LZMA_RA&quot;, verbose=FALSE) We can interact with the GDS file using the SeqArray package. gds &lt;- seqOpen(gdsfile) gds ## Object of class &quot;SeqVarGDSClass&quot; ## File: /projects/users/stephanie/Code/TOPMed/topmed_workshop_2017/1KG_phase3_subset_chr1.gds (70.3K) ## + [ ] * ## |--+ description [ ] * ## |--+ sample.id { Str8 1126 LZMA_ra(9.66%), 877B } * ## |--+ variant.id { Int32 1120 LZMA_ra(17.5%), 793B } * ## |--+ position { Int32 1120 LZMA_ra(78.5%), 3.4K } * ## |--+ chromosome { Str8 1120 LZMA_ra(4.55%), 109B } * ## |--+ allele { Str8 1120 LZMA_ra(26.0%), 1.2K } * ## |--+ genotype [ ] * ## | |--+ data { Bit2 2x1126x1121 LZMA_ra(8.34%), 51.4K } * ## | |--+ extra.index { Int32 3x0 LZMA_ra, 18B } * ## | \\--+ extra { Int16 0 LZMA_ra, 18B } ## |--+ phase [ ] ## | |--+ data { Bit1 1126x1120 LZMA_ra(0.11%), 177B } * ## | |--+ extra.index { Int32 3x0 LZMA_ra, 18B } * ## | \\--+ extra { Bit1 0 LZMA_ra, 18B } ## |--+ annotation [ ] ## | |--+ id { Str8 1120 LZMA_ra(40.4%), 3.6K } * ## | |--+ qual { Float32 1120 LZMA_ra(2.46%), 117B } * ## | |--+ filter { Int32,factor 1120 LZMA_ra(2.46%), 117B } * ## | |--+ info [ ] ## | \\--+ format [ ] ## \\--+ sample.annotation [ ] # the unique sample identifier comes from the VCF header sample.id &lt;- seqGetData(gds, &quot;sample.id&quot;) length(sample.id) ## [1] 1126 head(sample.id) ## [1] &quot;HG00096&quot; &quot;HG00097&quot; &quot;HG00099&quot; &quot;HG00100&quot; &quot;HG00101&quot; &quot;HG00102&quot; # a unique integer ID is assigned to each variant variant.id &lt;- seqGetData(gds, &quot;variant.id&quot;) length(variant.id) ## [1] 1120 head(variant.id) ## [1] 1 2 3 4 5 6 # reference allele frequency of each variant afreq &lt;- seqAlleleFreq(gds) hist(afreq, breaks=50) # define a filter to read a subset of data seqSetFilter(gds, variant.id=1:10, sample.id=sample.id[1:5]) ## # of selected samples: 5 ## # of selected variants: 10 # genotype data is stored as number of copies of each allele geno &lt;- seqGetData(gds, &quot;genotype&quot;) dim(geno) ## [1] 2 5 10 geno[,,1:2] ## , , 1 ## ## sample ## allele [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 0 0 0 ## [2,] 0 0 0 0 0 ## ## , , 2 ## ## sample ## allele [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 0 0 0 ## [2,] 0 0 0 0 0 The SeqVarTools package has some additional functions for interacting with SeqArray-format GDS files. library(SeqVarTools) # return genotypes in matrix format getGenotype(gds) ## variant ## sample 1 2 3 4 5 6 7 8 9 10 ## HG00096 &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; ## HG00097 &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; ## HG00099 &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; ## HG00100 &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; ## HG00101 &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; getGenotypeAlleles(gds) ## variant ## sample 1 2 3 4 5 6 7 8 9 10 ## HG00096 &quot;C|C&quot; &quot;C|C&quot; &quot;C|C&quot; &quot;C|C&quot; &quot;T|T&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;A|A&quot; &quot;A|A&quot; &quot;C|C&quot; ## HG00097 &quot;C|C&quot; &quot;C|C&quot; &quot;C|C&quot; &quot;C|C&quot; &quot;T|T&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;A|A&quot; &quot;A|A&quot; &quot;C|C&quot; ## HG00099 &quot;C|C&quot; &quot;C|C&quot; &quot;C|C&quot; &quot;C|C&quot; &quot;T|T&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;A|A&quot; &quot;A|A&quot; &quot;C|C&quot; ## HG00100 &quot;C|C&quot; &quot;C|C&quot; &quot;C|C&quot; &quot;C|C&quot; &quot;T|T&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;A|A&quot; &quot;A|A&quot; &quot;C|C&quot; ## HG00101 &quot;C|C&quot; &quot;C|C&quot; &quot;C|C&quot; &quot;C|C&quot; &quot;T|T&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;A|A&quot; &quot;A|A&quot; &quot;C|C&quot; refDosage(gds) ## variant ## sample 1 2 3 4 5 6 7 8 9 10 ## HG00096 2 2 2 2 2 2 2 2 2 2 ## HG00097 2 2 2 2 2 2 2 2 2 2 ## HG00099 2 2 2 2 2 2 2 2 2 2 ## HG00100 2 2 2 2 2 2 2 2 2 2 ## HG00101 2 2 2 2 2 2 2 2 2 2 altDosage(gds) ## variant ## sample 1 2 3 4 5 6 7 8 9 10 ## HG00096 0 0 0 0 0 0 0 0 0 0 ## HG00097 0 0 0 0 0 0 0 0 0 0 ## HG00099 0 0 0 0 0 0 0 0 0 0 ## HG00100 0 0 0 0 0 0 0 0 0 0 ## HG00101 0 0 0 0 0 0 0 0 0 0 # look at reference and alternate alleles refChar(gds) ## [1] &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;T&quot; &quot;G&quot; &quot;G&quot; &quot;A&quot; &quot;A&quot; &quot;C&quot; altChar(gds) ## [1] &quot;G&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;T&quot; &quot;C&quot; &quot;T&quot; # reset the filter to all variants and samples seqResetFilter(gds) ## # of selected samples: 1,126 ## # of selected variants: 1,120 # how many alleles for each variant? n &lt;- seqNumAllele(gds) table(n) ## n ## 2 3 4 ## 1099 20 1 # some variants have multiple alleles multi.allelic &lt;- which(n &gt; 2) altChar(gds)[multi.allelic] ## [1] &quot;GT,G&quot; &quot;G,T&quot; &quot;A,T&quot; ## [4] &quot;A,T&quot; &quot;ATG,ATGTG&quot; &quot;C,G&quot; ## [7] &quot;A,T&quot; &quot;C,T&quot; &quot;A,C&quot; ## [10] &quot;TAA,T&quot; &quot;GTTA,GTTT&quot; &quot;GCC,GCCC,G&quot; ## [13] &quot;A,C&quot; &quot;A,C&quot; &quot;A,C&quot; ## [16] &quot;CAAGCAT,CGAGCAT&quot; &quot;CATTATT,C&quot; &quot;AT,C&quot; ## [19] &quot;TGTGA,C&quot; &quot;CCATT,CCATTCATT&quot; &quot;C,G&quot; # extract a particular alternate allele altChar(gds, n=1)[multi.allelic] ## [1] &quot;GT&quot; &quot;G&quot; &quot;A&quot; &quot;A&quot; &quot;ATG&quot; &quot;C&quot; &quot;A&quot; ## [8] &quot;C&quot; &quot;A&quot; &quot;TAA&quot; &quot;GTTA&quot; &quot;GCC&quot; &quot;A&quot; &quot;A&quot; ## [15] &quot;A&quot; &quot;CAAGCAT&quot; &quot;CATTATT&quot; &quot;AT&quot; &quot;TGTGA&quot; &quot;CCATT&quot; &quot;C&quot; altChar(gds, n=2)[multi.allelic] ## [1] &quot;G&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;ATGTG&quot; ## [6] &quot;G&quot; &quot;T&quot; &quot;T&quot; &quot;C&quot; &quot;T&quot; ## [11] &quot;GTTT&quot; &quot;GCCC&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; ## [16] &quot;CGAGCAT&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;CCATTCATT&quot; ## [21] &quot;G&quot; # how many variants are SNVs vs INDELs? table(isSNV(gds, biallelic=TRUE)) ## ## FALSE TRUE ## 110 1010 table(isSNV(gds, biallelic=FALSE)) ## ## FALSE TRUE ## 99 1021 seqClose(gds) 3.2 Exercise: HWE Use the hwe function in SeqVarTools to run a Hardy-Weinberg Equilibrium test on each variant. Identify a variant with low p-value and inspect its genotypes. "],
["computing-a-grm.html", "4 Computing a GRM", " 4 Computing a GRM We can use the SNPRelate package to compute a Genetic Relationship matrix (GRM). library(SeqArray) data.path &lt;- &quot;https://github.com/smgogarten/analysis_pipeline/raw/devel/testdata&quot; gdsfile &lt;- &quot;1KG_phase3_subset_chr1.gds&quot; if (!file.exists(gdsfile)) download.file(file.path(data.path, gdsfile), gdsfile) gds &lt;- seqOpen(gdsfile) library(SNPRelate) grm &lt;- snpgdsGRM(gds, method=&quot;GCTA&quot;) ## Genetic Relationship Matrix (GRM, GCTA): ## Calculating allele counts/frequencies ... ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed ## Excluding 13 SNVs (monomorphic: TRUE, MAF: NaN, missing rate: NaN) ## Working space: 1,126 samples, 1,107 SNVs ## using 1 (CPU) core ## CPU capabilities: Double-Precision SSE2 ## Thu Aug 3 12:00:37 2017 (internal increment: 680) ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed in 1s ## Thu Aug 3 12:00:38 2017 Done. names(grm) ## [1] &quot;sample.id&quot; &quot;snp.id&quot; &quot;grm&quot; dim(grm$grm) ## [1] 1126 1126 seqClose(gds) "],
["pc-relate.html", "5 PC-Relate 5.1 KING 5.2 PC-AiR 5.3 PC-Relate 5.4 Exercise", " 5 PC-Relate To disentangle ancestry from recent familial relatedness, we use the PC-Relate method. 5.1 KING Step 1 is to get initial estimates of kinship using KING, which is robust to population structure but not admixture. The KING algorithm is available in SNPRelate. Typically we select a subset of variants for this calculation with LD pruning. # use a GDS file with all chromosomes library(SeqArray) data.path &lt;- &quot;https://github.com/smgogarten/analysis_pipeline/raw/devel/testdata&quot; gdsfile &lt;- &quot;1KG_phase3_subset.gds&quot; if (!file.exists(gdsfile)) download.file(file.path(data.path, gdsfile), gdsfile) gds &lt;- seqOpen(gdsfile) # use a subset of 100 samples to make things run faster workshop.path &lt;- &quot;https://github.com/UW-GAC/topmed_workshop_2017/raw/master&quot; sampfile &lt;- &quot;samples_subset100.RData&quot; if (!file.exists(sampfile)) download.file(file.path(workshop.path, sampfile), sampfile) sample.id &lt;- TopmedPipeline::getobj(sampfile) # LD pruning to get variant set library(SNPRelate) snpset &lt;- snpgdsLDpruning(gds, sample.id=sample.id, method=&quot;corr&quot;, slide.max.bp=10e6, ld.threshold=sqrt(0.1)) ## SNV pruning based on LD: ## Excluding 1,120 SNVs on non-autosomes ## Calculating allele counts/frequencies ... ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed ## Excluding 13,673 SNVs (monomorphic: TRUE, MAF: NaN, missing rate: NaN) ## Working space: 100 samples, 10,967 SNVs ## using 1 (CPU) core ## sliding window: 10,000,000 basepairs, Inf SNPs ## |LD| threshold: 0.316228 ## method: correlation ## Chromosome 1: 31.16%, 349/1,120 ## Chromosome 2: 30.89%, 346/1,120 ## Chromosome 3: 31.07%, 348/1,120 ## Chromosome 4: 30.98%, 347/1,120 ## Chromosome 5: 29.46%, 330/1,120 ## Chromosome 6: 31.43%, 352/1,120 ## Chromosome 7: 28.12%, 315/1,120 ## Chromosome 8: 25.98%, 291/1,120 ## Chromosome 9: 27.86%, 312/1,120 ## Chromosome 10: 28.75%, 322/1,120 ## Chromosome 11: 26.79%, 300/1,120 ## Chromosome 12: 28.04%, 314/1,120 ## Chromosome 13: 25.27%, 283/1,120 ## Chromosome 14: 24.82%, 278/1,120 ## Chromosome 15: 22.50%, 252/1,120 ## Chromosome 16: 22.50%, 252/1,120 ## Chromosome 17: 21.61%, 242/1,120 ## Chromosome 18: 23.84%, 267/1,120 ## Chromosome 19: 21.61%, 242/1,120 ## Chromosome 20: 19.91%, 223/1,120 ## Chromosome 21: 17.95%, 201/1,120 ## Chromosome 22: 17.59%, 197/1,120 ## 6,363 markers are selected in total. sapply(snpset, length) ## chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 ## 349 346 348 347 330 352 315 291 312 322 300 314 ## chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 ## 283 278 252 252 242 267 242 223 201 197 pruned &lt;- unlist(snpset, use.names=FALSE) # KING king &lt;- snpgdsIBDKING(gds, sample.id=sample.id, snp.id=pruned) ## IBD analysis (KING method of moment) on genotypes: ## Calculating allele counts/frequencies ... ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed ## Working space: 100 samples, 6,363 SNVs ## using 1 (CPU) core ## No family is specified, and all individuals are treated as singletons. ## Relationship inference in the presence of population stratification. ## CPU capabilities: Double-Precision SSE2 ## Thu Aug 3 12:00:41 2017 (internal increment: 65536) ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed in 0s ## Thu Aug 3 12:00:41 2017 Done. names(king) ## [1] &quot;sample.id&quot; &quot;snp.id&quot; &quot;afreq&quot; &quot;IBS0&quot; &quot;kinship&quot; dim(king$kinship) ## [1] 100 100 kingMat &lt;- king$kinship colnames(kingMat) &lt;- rownames(kingMat) &lt;- king$sample.id We extract pairwise kinship estimates and IBS0 to plot. kinship &lt;- snpgdsIBDSelection(king) head(kinship) ## ID1 ID2 IBS0 kinship ## 1 HG00110 HG00116 0.02593116 -0.01874244 ## 2 HG00110 HG00120 0.02703127 -0.02932285 ## 3 HG00110 HG00128 0.02561685 -0.01725182 ## 4 HG00110 HG00136 0.02954581 -0.04383313 ## 5 HG00110 HG00137 0.02813138 -0.04292624 ## 6 HG00110 HG00141 0.02844570 -0.04201935 library(ggplot2) ggplot(kinship, aes(IBS0, kinship)) + geom_hline(yintercept=2^(-seq(3,9,2)/2), linetype=&quot;dashed&quot;, color=&quot;grey&quot;) + geom_point(alpha=0.5) + ylab(&quot;kinship estimate&quot;) + theme_bw() 5.2 PC-AiR The next step is PC-AiR, in which we select a set of unrelated samples that is maximally informative about all ancestries in the sample. We use this unrelated set for Principal Component Analysis (PCA), then project the relatives onto the PCs. First, we partition the samples into a related and unrelated set. We use a kinship threshold of degree 3 (unrelated is less than first cousins). We load the GENESIS package. In the first iteration, we use the KING estimates for both kinship (kinMat) and ancestry divergence (divMat). KING kinship estimates are negative for samples with different ancestry. library(GENESIS) sampset &lt;- pcairPartition(kinMat=kingMat, kin.thresh=2^(-9/2), divMat=kingMat, div.thresh=-2^(-9/2)) names(sampset) ## [1] &quot;rels&quot; &quot;unrels&quot; sapply(sampset, length) ## rels unrels ## 14 86 Typically we would repeat the LD pruning step on the set of unrelated samples we just identified, but for this example we will re-use the pruned set of variants from step 1. Using the SNPRelate package, we run PCA on the unrelated set and project values for the related set. # run PCA on unrelated set pca.unrel &lt;- snpgdsPCA(gds, sample.id=sampset$unrels, snp.id=pruned) ## Principal Component Analysis (PCA) on genotypes: ## Calculating allele counts/frequencies ... ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed ## Excluding 222 SNVs (monomorphic: TRUE, MAF: NaN, missing rate: NaN) ## Working space: 86 samples, 6,141 SNVs ## using 1 (CPU) core ## CPU capabilities: Double-Precision SSE2 ## Thu Aug 3 12:00:44 2017 (internal increment: 8952) ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed in 0s ## Thu Aug 3 12:00:44 2017 Begin (eigenvalues and eigenvectors) ## Thu Aug 3 12:00:44 2017 Done. # project values for relatives snp.load &lt;- snpgdsPCASNPLoading(pca.unrel, gdsobj=gds) ## SNP loading: ## Working space: 86 samples, 6141 SNPs ## using 1 (CPU) core ## using the top 32 eigenvectors ## Thu Aug 3 12:00:44 2017 (internal increment: 65536) ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed in 0s ## Thu Aug 3 12:00:44 2017 Done. samp.load &lt;- snpgdsPCASampLoading(snp.load, gdsobj=gds, sample.id=sampset$rels) ## Sample loading: ## Working space: 14 samples, 6141 SNPs ## using 1 (CPU) core ## using the top 32 eigenvectors ## Thu Aug 3 12:00:44 2017 (internal increment: 65536) ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed in 0s ## Thu Aug 3 12:00:44 2017 Done. # combine unrelated and related PCs and order as in GDS file pcs &lt;- rbind(pca.unrel$eigenvect, samp.load$eigenvect) rownames(pcs) &lt;- c(pca.unrel$sample.id, samp.load$sample.id) samp.ord &lt;- match(sample.id, rownames(pcs)) pcs &lt;- pcs[samp.ord,] We need to determine which PCs are ancestry informative. To do this we need population information for the 1000 Genomes samples. This information is stored in an AnnotatedDataFrame, which is a data.frame with optional metadata describing the colunms. The class is defined in the Biobase package. We load the stored object using the getobj function from the TopmedPipeline package. library(Biobase) sampfile &lt;- &quot;1KG_phase3_subset_annot.RData&quot; if (!file.exists(sampfile)) download.file(file.path(data.path, sampfile), sampfile) annot &lt;- TopmedPipeline::getobj(sampfile) annot ## An object of class &#39;AnnotatedDataFrame&#39; ## rowNames: 1 2 ... 2504 (1126 total) ## varLabels: sample.id family.id ... status (7 total) ## varMetadata: labelDescription head(pData(annot)) ## sample.id family.id Population Population.Description sex ## 1 HG00096 HG00096 GBR British in England and Scotland M ## 2 HG00097 HG00097 GBR British in England and Scotland F ## 3 HG00099 HG00099 GBR British in England and Scotland F ## 4 HG00100 HG00100 GBR British in England and Scotland F ## 5 HG00101 HG00101 GBR British in England and Scotland M ## 6 HG00102 HG00102 GBR British in England and Scotland F ## outcome status ## 1 8.580417 0 ## 2 10.769515 1 ## 3 10.428110 0 ## 4 11.034979 1 ## 5 9.706791 0 ## 6 10.225198 0 varMetadata(annot) ## labelDescription ## sample.id sample identifier ## family.id family identifier ## Population population abbreviation ## Population.Description population description ## sex sex ## outcome simulated random normal phenotype ## status simulated case/control status We make a parallel coordinates plot, color-coding by 1000 Genomes population. We load the dplyr package for data.frame manipulation. pc.df &lt;- as.data.frame(pcs) names(pc.df) &lt;- 1:ncol(pcs) pc.df$sample.id &lt;- row.names(pcs) library(dplyr) annot &lt;- pData(annot) %&gt;% select(sample.id, Population) pc.df &lt;- left_join(pc.df, annot, by=&quot;sample.id&quot;) library(GGally) library(RColorBrewer) pop.cols &lt;- setNames(brewer.pal(12, &quot;Paired&quot;), c(&quot;ACB&quot;, &quot;ASW&quot;, &quot;CEU&quot;, &quot;GBR&quot;, &quot;CHB&quot;, &quot;JPT&quot;, &quot;CLM&quot;, &quot;MXL&quot;, &quot;LWK&quot;, &quot;YRI&quot;, &quot;GIH&quot;, &quot;PUR&quot;)) ggparcoord(pc.df, columns=1:12, groupColumn=&quot;Population&quot;, scale=&quot;uniminmax&quot;) + scale_color_manual(values=pop.cols) + xlab(&quot;PC&quot;) + ylab(&quot;&quot;) 5.3 PC-Relate The first 2 PCs separate populations, so we use them to compute kinship estimates adjusting for ancestry. The PC-Relate function expects a SeqVarData object, which allows linking sample and variant annotation with a GDS file in a single object. We will cover these in more detail later for association testing, but for now we create a bare object with no annotation. seqResetFilter(gds, verbose=FALSE) library(SeqVarTools) seqData &lt;- SeqVarData(gds) pcrel &lt;- pcrelate(seqData, pcMat=pcs[,1:2], training.set=sampset$unrels, scan.include=sample.id, snp.include=pruned) names(pcrel) ## [1] &quot;sample.id&quot; &quot;kinship&quot; &quot;ibd.probs&quot; &quot;nsnp&quot; &quot;kincorrect&quot; ## [6] &quot;k2correct&quot; &quot;call&quot; &quot;freq.type&quot; &quot;scale&quot; PC-Relate is an iterative method. Now that we have ancestry-adjusted kinship estimates, we can use them to better adjust for ancestry in the PCs. This time we use the pcair function, which combines partitioning the sample set and running PCA in one step. First we need to make a kinship matrix from the PC-Relate results. The KING matrix is still used for ancestry divergence. pcrelMat &lt;- pcrelateMakeGRM(pcrel, scaleKin=1) pca &lt;- pcair(seqData, v=32, kinMat=pcrelMat, kin.thresh=2^(-9/2), divMat=kingMat, div.thresh=-2^(-9/2), scan.include=sample.id, snp.include=pruned) names(pca) ## [1] &quot;vectors&quot; &quot;values&quot; &quot;sum.values&quot; &quot;rels&quot; &quot;unrels&quot; ## [6] &quot;kin.thresh&quot; &quot;div.thresh&quot; &quot;nsamp&quot; &quot;nsnps&quot; &quot;MAF&quot; ## [11] &quot;call&quot; &quot;method&quot; pcs &lt;- pca$vectors pc.df &lt;- as.data.frame(pcs) names(pc.df) &lt;- paste0(&quot;PC&quot;, 1:ncol(pcs)) pc.df$sample.id &lt;- row.names(pcs) pc.df &lt;- left_join(pc.df, annot, by=&quot;sample.id&quot;) ggplot(pc.df, aes(PC1, PC2, color=Population)) + geom_point() + scale_color_manual(values=pop.cols) Now we use the revised PCs to compute new kinship estimates. One can run the iteration multiple times and check for conversion, but usually two rounds are sufficient. pcrel &lt;- pcrelate(seqData, pcMat=pcs[,1:2], training.set=pca$unrels, scan.include=sample.id, snp.include=pruned) We plot the kinship estimates from PC-Relate, and notice that the values for less related pairs are much better behaved. kinship &lt;- pcrelateReadKinship(pcrel) ggplot(kinship, aes(k0, kin)) + geom_hline(yintercept=2^(-seq(3,9,2)/2), linetype=&quot;dashed&quot;, color=&quot;grey&quot;) + geom_point(alpha=0.5) + ylab(&quot;kinship estimate&quot;) + theme_bw() seqClose(gds) 5.4 Exercise Complete one round of iteration using all samples from the test dataset and plot the results. "],
["finding-topmed-study-phenotypes-on-dbgap.html", "6 Finding TOPMed Study Phenotypes on dbGaP", " 6 Finding TOPMed Study Phenotypes on dbGaP "],
["association-tests.html", "7 Association tests 7.1 Null model 7.2 Single-variant tests 7.3 Sliding window tests 7.4 Exercise: logistic regression 7.5 Aggregate tests", " 7 Association tests Since TOPMed has many studies with related participants, we focus on linear mixed models. Logistic mixed models are also possible using GENESIS, see the GMMAT paper. 7.1 Null model The first step in an association test is to fit the null model. We use the AnnotatedDataFrame with phenotypes, and a GRM. If the sample set involves multiple distinct groups with different variances for the phenotype, we recommend allowing the model to use heterogeneous variance among groups. data.path &lt;- &quot;https://github.com/smgogarten/analysis_pipeline/raw/devel/testdata&quot; sampfile &lt;- &quot;1KG_phase3_subset_annot.RData&quot; if (!file.exists(sampfile)) download.file(file.path(data.path, sampfile), sampfile) annot &lt;- TopmedPipeline::getobj(sampfile) grmfile &lt;- &quot;grm.RData&quot; if (!file.exists(grmfile)) download.file(file.path(data.path, grmfile), grmfile) grm &lt;- TopmedPipeline::getobj(grmfile) rownames(grm$grm) &lt;- colnames(grm$grm) &lt;- grm$sample.id library(GENESIS) nullmod &lt;- fitNullMM(annot, outcome=&quot;outcome&quot;, covars=c(&quot;sex&quot;, &quot;Population&quot;), covMatList=grm$grm, group.var=&quot;Population&quot;, verbose=FALSE) We also recommend taking an inverse normal transform of the residuals and refitting the model. This is done separately for each group, and the transformed residuals are rescaled. See the full procedure in the pipeline documenation. 7.2 Single-variant tests Single-variant tests are the same as in GWAS. We use the assocTestMM function in GENESIS. We have to create a SeqVarData object including both the GDS file and the sample annotation containing phenotypes. library(SeqVarTools) gdsfile &lt;- &quot;1KG_phase3_subset_chr1.gds&quot; if (!file.exists(gdsfile)) download.file(file.path(data.path, gdsfile), gdsfile) gds &lt;- seqOpen(gdsfile) seqData &lt;- SeqVarData(gds, sampleData=annot) assoc &lt;- assocTestMM(seqData, nullmod) head(assoc) ## snpID chr n MAF minor.allele Est SE ## 1 1 1 1126 0.0039964476 alt 0.037896432 0.3547545 ## 2 2 1 1126 0.0492895204 alt 0.173271721 0.1038710 ## 3 3 1 1126 0.0004440497 alt 0.034881381 1.0211652 ## 4 4 1 1126 0.0008880995 alt 0.003698581 0.6811618 ## 5 5 1 1126 0.0071047957 alt -0.062695115 0.2685319 ## 6 6 1 1126 0.0022202487 alt 0.574104228 0.4504458 ## Wald.Stat Wald.pval ## 1 1.141145e-02 0.91492830 ## 2 2.782705e+00 0.09528713 ## 3 1.166797e-03 0.97275083 ## 4 2.948287e-05 0.99566766 ## 5 5.450992e-02 0.81539366 ## 6 1.624413e+00 0.20247756 We make a QQ plot to examine the results. library(ggplot2) qqPlot &lt;- function(pval) { pval &lt;- pval[!is.na(pval)] n &lt;- length(pval) x &lt;- 1:n dat &lt;- data.frame(obs=sort(pval), exp=x/n, upper=qbeta(0.025, x, rev(x)), lower=qbeta(0.975, x, rev(x))) ggplot(dat, aes(-log10(exp), -log10(obs))) + geom_line(aes(-log10(exp), -log10(upper)), color=&quot;gray&quot;) + geom_line(aes(-log10(exp), -log10(lower)), color=&quot;gray&quot;) + geom_point() + geom_abline(intercept=0, slope=1, color=&quot;red&quot;) + xlab(expression(paste(-log[10], &quot;(expected P)&quot;))) + ylab(expression(paste(-log[10], &quot;(observed P)&quot;))) + theme_bw() } qqPlot(assoc$Wald.pval) 7.3 Sliding window tests For rare variants, we can do burden tests or SKAT on sliding windows using the GENESIS function assocTestSeqWindow. We restrict the test to variants with alternate allele frequency &lt; 0.1. (For real data, this threshold would be lower.) We use a flat weighting scheme. assoc &lt;- assocTestSeqWindow(seqData, nullmod, test=&quot;Burden&quot;, AF.range=c(0,0.1), weight.beta=c(1,1), window.size=5, window.shift=2) names(assoc) ## [1] &quot;param&quot; &quot;window&quot; &quot;nsample&quot; &quot;results&quot; &quot;variantInfo&quot; head(assoc$results) ## chr window.start window.stop n.site dup burden.skew Score ## 1 1 966001 971000 1 0 11.036036 0.30138968 ## 2 1 968001 973000 1 1 11.036036 0.30138968 ## 3 1 970001 975000 1 1 11.036036 0.30138968 ## 4 1 982001 987000 1 0 3.041979 16.03409196 ## 5 1 984001 989000 1 1 3.041979 16.03409196 ## 6 1 1022001 1027000 1 0 33.466573 0.03348047 ## Var Score.stat Score.pval ## 1 7.9529830 0.011421594 0.91489064 ## 2 7.9529830 0.011421594 0.91489064 ## 3 7.9529830 0.011421594 0.91489064 ## 4 92.5372698 2.778254702 0.09555224 ## 5 92.5372698 2.778254702 0.09555224 ## 6 0.9598379 0.001167845 0.97273860 head(assoc$variantInfo) ## variantID allele chr pos n.obs freq weight ## 1 1 1 1 970546 1126 0.0039964476 1 ## 2 2 1 1 985900 1126 0.0492895204 1 ## 3 3 1 1 1025045 1126 0.0004440497 1 ## 4 4 1 1 1265550 1126 0.0008880995 1 ## 5 5 1 1 1472676 1126 0.0071047957 1 ## 6 6 1 1 1735725 1126 0.0022202487 1 qqPlot(assoc$results$Score.pval) For SKAT, we use the Wu weights. assoc &lt;- assocTestSeqWindow(seqData, nullmod, test=&quot;SKAT&quot;, AF.range=c(0,0.1), weight.beta=c(1,25), window.size=5, window.shift=2) head(assoc$results) ## chr window.start window.stop n.site dup Q_0 pval_0 err_0 ## 1 1 966001 971000 1 0 4.684458e+01 0.91489064 0 ## 2 1 968001 973000 1 1 4.684458e+01 0.91489064 0 ## 3 1 970001 975000 1 1 4.684458e+01 0.91489064 0 ## 4 1 982001 987000 1 0 1.419993e+04 0.09555224 0 ## 5 1 984001 989000 1 1 1.419993e+04 0.09555224 0 ## 6 1 1022001 1027000 1 0 6.858109e-01 0.97273860 0 head(assoc$variantInfo) ## variantID allele chr pos n.obs freq weight ## 1 1 1 1 970546 1126 0.0039964476 22.709172 ## 2 2 1 1 985900 1126 0.0492895204 7.431881 ## 3 3 1 1 1025045 1126 0.0004440497 24.734926 ## 4 4 1 1 1265550 1126 0.0008880995 24.472547 ## 5 5 1 1 1472676 1126 0.0071047957 21.067933 ## 6 6 1 1 1735725 1126 0.0022202487 23.701317 qqPlot(assoc$results$pval_0) 7.4 Exercise: logistic regression fitNullMM can use a binary phenotype as the outcome variable by specifying the argument family=binomial. Use the status column in the sample annotation to fit a null model for simulated case/control status. Then run a single-variant test and a sliding window test using this model. 7.5 Aggregate tests 7.5.1 Variant annotation Rare variants are generally aggregated into some meaningful units for association testing to decrease multiple testing burden and increase statistical power. Various genomic and epigenomic annotations can be used to define aggregation units and filter them. A large number of annotations are available through the Whole Genome Sequence Annotator (WGSA) to the TOPMed users. 7.5.2 Defining aggregate units We will be using a gene-based aggregation unit, where each unit is a GENCODE gene and 20 kb flanking region upstream and downstream of it. For real data, one will likely filter variants within each unit based on various annotations (examples include loss of function, conservation, deleteriousness scores, etc.). The aggregation units are defined in an R dataframe. Each row of the dataframe specifies a variant (chromosome, position, ref, alt) and the group identifier (group_id) assigned to it. Mutiple rows with different group identifiers can be specified to assign a variant to different groups (for example a variant can be assigned to mutiple genes). aggfile &lt;- &quot;variants_by_gene.RData&quot; if (!file.exists(aggfile)) download.file(file.path(workshop.path, aggfile), aggfile) aggunit &lt;- TopmedPipeline::getobj(aggfile) names(aggunit) ## [1] &quot;group_id&quot; &quot;chromosome&quot; &quot;position&quot; &quot;ref&quot; &quot;alt&quot; head(aggunit) ## # A tibble: 6 x 5 ## group_id chromosome position ref alt ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 ENSG00000000005.5 X 99850725 A G ## 2 ENSG00000000938.8 1 27960254 A G ## 3 ENSG00000001084.6 6 53357691 C T ## 4 ENSG00000001084.6 6 53413986 T C ## 5 ENSG00000001084.6 6 53466979 C T ## 6 ENSG00000001167.10 6 41064020 A G # an example of variant that is present in mutiple groups library(dplyr) mult &lt;- aggunit %&gt;% group_by(chromosome, position) %&gt;% summarise(n=n()) %&gt;% filter(n &gt; 1) inner_join(aggunit, mult[2,1:2]) ## # A tibble: 4 x 5 ## group_id chromosome position ref alt ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 ENSG00000188157.9 1 985900 C T ## 2 ENSG00000217801.5 1 985900 C T ## 3 ENSG00000242590.1 1 985900 C T ## 4 ENSG00000273443.1 1 985900 C T 7.5.3 Association testing with aggregate units We can run a burden test or SKAT on each of these units using the GENESIS function assocTestSeq. This function expects a list, where each element of the list is a dataframe representing a single aggregation unit and containing the unique variant.id assigned to each variant in a GDS file. We use the TopmedPipeline function aggregateListByAllele to quickly convert our single dataframe to the required format. This function can account for multiallelic variants (the same chromosome, position, and ref, but different alt alleles). The first argument is the GDS object returned by seqOpen (see above). library(TopmedPipeline) aggVarList &lt;- aggregateListByAllele(gds, aggunit) length(aggVarList) ## [1] 932 head(names(aggVarList)) ## [1] &quot;ENSG00000188157.9&quot; &quot;ENSG00000242590.1&quot; &quot;ENSG00000217801.5&quot; ## [4] &quot;ENSG00000273443.1&quot; &quot;ENSG00000131591.13&quot; &quot;ENSG00000237330.2&quot; aggVarList[[1]] ## variant.id chromosome position ref nAlleles allele allele.index ## 1 1 1 970546 C 2 G 1 ## 2 2 1 985900 C 2 T 1 As in the previous section, we must fit the null model before running the association test. assoc &lt;- assocTestSeq(seqData, nullmod, test=&quot;Burden&quot;, aggVarList=aggVarList, AF.range=c(0,0.1), weight.beta=c(1,1)) names(assoc) ## [1] &quot;param&quot; &quot;nsample&quot; &quot;results&quot; &quot;variantInfo&quot; head(assoc$results) ## n.site n.sample.alt burden.skew Score Var ## ENSG00000188157.9 2 116 2.867095 16.33548164 99.2100284 ## ENSG00000242590.1 2 116 2.867095 16.33548164 99.2100284 ## ENSG00000217801.5 1 107 3.041979 16.03409196 92.5372698 ## ENSG00000273443.1 1 107 3.041979 16.03409196 92.5372698 ## ENSG00000131591.13 1 1 33.466573 0.03348047 0.9598379 ## ENSG00000237330.2 1 1 33.466573 0.03348047 0.9598379 ## Score.stat Score.pval ## ENSG00000188157.9 2.689727688 0.10099707 ## ENSG00000242590.1 2.689727688 0.10099707 ## ENSG00000217801.5 2.778254702 0.09555224 ## ENSG00000273443.1 2.778254702 0.09555224 ## ENSG00000131591.13 0.001167845 0.97273860 ## ENSG00000237330.2 0.001167845 0.97273860 head(names(assoc$variantInfo)) ## [1] &quot;ENSG00000188157.9&quot; &quot;ENSG00000242590.1&quot; &quot;ENSG00000217801.5&quot; ## [4] &quot;ENSG00000273443.1&quot; &quot;ENSG00000131591.13&quot; &quot;ENSG00000237330.2&quot; head(assoc$variantInfo[[1]]) ## variantID allele chr pos n.obs freq weight ## 1 1 1 1 970546 1126 0.003996448 1 ## 2 2 1 1 985900 1126 0.049289520 1 qqPlot(assoc$results$Score.pval) seqClose(gds) 7.5.4 Exercise Since we are working with a subset of the data, many of the genes listed in group_id have a very small number of variants. Create a new set of units combining adjacent genes, and run SKAT using those units. "],
["analysis-pipeline.html", "8 Analysis Pipeline 8.1 Running on a local cluster 8.2 Running on AWS Batch", " 8 Analysis Pipeline The DCC’s analysis pipeline is hosted on github: https://github.com/smgogarten/analysis_pipeline 8.1 Running on a local cluster To run a burden test on our local SGE cluster, first we create a config file and call it assoc_window_burden.config: out_prefix &quot;test&quot; gds_file &quot;testdata/1KG_phase3_subset_chr .gds&quot; phenotype_file &quot;testdata/1KG_phase3_subset_annot.RData&quot; pcrelate_file &quot;testdata/round2_pcrelate.gds&quot; pca_file &quot;testdata/round2_pcair.RData&quot; sample_include_file &quot;testdata/sample_include.RData&quot; variant_include_file &quot;testdata/variant_include_chr .RData&quot; outcome outcome covars &quot;sex&quot; n_pcs 4 alt_freq_range &quot;0 0.1&quot; test &quot;burden&quot; test_type &quot;score&quot; We will use the python script assoc.py to submit all jobs. First we look at the available options: setenv PIPELINE /projects/topmed/working_code/analysis_pipeline $PIPELINE/assoc.py --help Let’s run a sliding window test on chromosomes 1-10. We will also specify the cluster type, although UW_Cluster is actually the default. The last argument is our config file. First, we print the commands that will be be run without actually submitting jobs: $PIPELINE/assoc.py --chromosomes 1-10 --cluster_type UW_Cluster --print_only window testdata/assoc_window_burden.config The default segment length is 10,000 kb, but we can change that to 50,000 kb when we submit: $PIPELINE/assoc.py --chromosomes 1-10 --cluster_type UW_Cluster --segment_length 50000 window testdata/assoc_window_burden.config We can use the qstat command to check the status of our jobs. 8.2 Running on AWS Batch To run a burden test on AWS Batch, we do the following general steps: 1. Log into the the docker AMI instance 2. cd to a working directory on our EFS volume 3. Create the configuration file assoc_window_burden.config 4. Optionally execute the association pipeline specifying the AWS Batch service to print out the commands (not running the pipeline) 5. Execute the association pipeline specifying the AWS Batch service to run the pipeline 6. Monitor the pipeline via the AWS Batch console 8.2.1 Log into AWS docker image ssh into our image which is running docker. Various docker commands can be executed including running TOPMed version of R (note: TOPMed data is not part of the docker image). ssh -i ~/.ssh/&lt;some private key&gt; kuraisa@54.244.25.94 [kuraisa@ip-172-255-46-100]~ _4816$ docker images ... [kuraisa@ip-172-255-46-100]~ _4817$ docker run -it uwgac/r-topmed:dev /bin/bash /# which R ... /# R ... &gt; .libPaths() ... &gt; library(SeqArray) ... &gt; q() ... /# exit [kuraisa@ip-172-255-46-100]~ _4818$ 8.2.2 cd to working directory and create config file cd /projects/topmed/analysts/kuraisa/tm-workshop/ vi assoc_window_burden.config ... 8.2.3 Print out AWS commands if executing the pipeline python /projects/topmed/dev_code/analysis_pipeline/assoc.py \\ single ./assoc_window_burden.config \\ --cluster_type AWS_Batch --verbose \\ --cluster_file \\ /projects/topmed/dev_code/analysis_pipeline/testdata_batch.json \\ --print &gt; single_print.log 2&gt;&amp;1 8.2.4 Execute the pipeline python /projects/topmed/dev_code/analysis_pipeline/assoc.py \\ single ./assoc_window_burden.config \\ --cluster_type AWS_Batch --verbose \\ --cluster_file \\ /projects/topmed/dev_code/analysis_pipeline/testdata_batch.json \\ &gt; burden_print.log 2&gt;&amp;1 8.2.5 Monitor the jobs From the web browser, log into the AWS account and select the Batch Services to monitor: - Summary via Dashboard - Job queue Optimal_topmed_testdata - View high-level job logs You can switch to ec2 services to monitor instances being created or running to support the various jobs. "],
["analysis-commons.html", "9 Analysis Commons 9.1 Outline 9.2 Web Interface and Running an Analysis Application 9.3 Command line interface 9.4 Writing your own Apps", " 9 Analysis Commons 9.1 Outline Introduction to web-interface Running a single variant analysis Workflows and monitoring jobs Running aggregate tests (SKAT) Run batch jobs from the command line Writing your own Apps 9.2 Web Interface and Running an Analysis Application 9.2.1 Exercise 1) Run a single variant analysis. Note that the job will finish instantaneously if you don’t change the output file name. It knows that you are running the exact same job and will just reuse results from previous analyses. Log into http://dnanexus.com using the user name and password listed on the handout. Should be in the form of Username:topmed_# and Password:Topmed_#. Ignore warning about default billing account. Navigate to and select (dcc:tools/genesis_v0.7) File inputs: * phenofile -&gt; phenotype/1KG_pheno.csv * genotypefile -&gt; genotypes/1KG_phase3_subset_chr1.gds * kinship -&gt; kinship/1KG_kins.Rda * Note: orange aggregation, annotation and genefile can be left empty Parameter inputs: * output folder: output/YOURFOLDERNAME * outcome (Column name of the outcome variable): outcome * covariates (case sepecific): Population,sex * prefix for output filename: single_chr1 * test_type: Single * pheno_id: sample.id * Note: Other options can be left as their defaults, some are only used for aggreagate tests 9.2.2 Exercise 2) Run SKAT test grouping variants into gene transcript regions and limit the variants to those with a CADD phred score &gt; 2 and MAF &lt;= 5%. Italic inputs below are the same as single variant; update the parameters &amp; files to change to a SKAT test. Go to the monitor tab. Click on the Name of a job ( or someone’s ) that successfully completed the single variant analysis, then click “Launch as new Job” and modify the inputs. File inputs: * phenofile -&gt; phenotype/1KG_pheno.csv * genotypefile -&gt; genotypes/1KG_phase3_subset_chr1.gds * kinship -&gt; kinship/1KG_kins.Rda * annotation -&gt; annotation/1KG_annotation_CHR1.txt * genefile -&gt; aggregation/AggUnit_CHR1_ucscgene.csv Parameter inputs: * outcome: outcome * covariates: Population,sex * pheno_id: sample.id * output folder: output/YOURFOLDERNAME * outputfilename: skat_chr1_geneBased_CADDgt2 * test_type: SKAT * snp_filter: CADD_phred&gt;2 * min_mac:0 * top_maf: 0.05 * weights: c(1,25) 9.3 Command line interface References: * Command Line Interface Quickstart * Index of dx commands 9.3.1 Log in to AWI Replace topmed_## with the user ID from your handout $ ssh topmed_##@34.212.243.167 --timeout 2h You will be prompted for your password, e.g. Topmed_## (Note capitolization) _Please ignore login warnings $ source /usr/local/dx-toolkit/environment $ dx login Enter the following at the prompts username: topmed_## password: Topmed_## project:dcc ( type 0 to select dcc ) You can select or change project once you are logged in $ dx select dcc 9.3.2 Exercise 3) Navigate directories, make output directory, examine files File paths: &lt;project&gt;:/path/to/file.txt Example: dcc:/phenotypes/1KG_pheno.csv List directory contents: $ dx select dcc $ dx ls $ dx ls /tools $ dx ls dcc:/tools Get results from project $ dx download dcc:/phenotype/1KG_pheno.csv $ ls $ head 1KG_pheno.csv 9.3.3 Exercise 4) Run single variant analysis from command line using bash script Open the single_multichrom.sh bash script and edit to replace the output directory “YOURNAME” to your folder $ dx describe tools/genesis_v0.7 Either edit using nano $ nano single_multichrom.sh Run the App. Will loop over 2 chromosomes running the single variant analyses $ ./single_multichrom.sh 9.4 Writing your own Apps 9.4.1 Exercise 5) Write an App that creates phenotype residuals and performs an inverse normal transform Use app wizard to create template $ dx-app-wizard App Name: make_residuals Title []: Create inverse normal transformed residuals 1st input name (&lt;ENTER&gt; to finish): phenofile Label (optional human-readable name) []: CSV phenotype file Choose a class (&lt;TAB&gt; twice for choices): file This is an optional parameter [y/n]: n 2nd input name (&lt;ENTER&gt; to finish): model Label (optional human-readable name) []: model for creating residuals (e.g. outcome~age+Population ) Choose a class (&lt;TAB&gt; twice for choices): string This is an optional parameter [y/n]: n 3rd input name (&lt;ENTER&gt; to finish): prefix Label (optional human-readable name) []: Output filename prefix Choose a class (&lt;TAB&gt; twice for choices): string This is an optional parameter [y/n]: n 4th input name (&lt;ENTER&gt; to finish): &lt;ENTER&gt; 1st output name (&lt;ENTER&gt; to finish): output Label (optional human-readable name) []: Choose a class (&lt;TAB&gt; twice for choices): file Timeout policy [48h]: 1h Programming language: bash *Use defaults for other options* Look at the files created by the wizard cd make_residuals/ ls more dxapp.json Edit App executable to run an R script $ vi src/make_residuals.sh main() { echo &quot;Value of phenofile: &#39;$phenofile&#39;&quot; echo &quot;Value of model: &#39;$model&#39;&quot; echo &quot;Value of prefix: &#39;$prefix&#39;&quot; dx download &quot;$phenofile&quot; -o phenofile Rscript /make_resid.R $model output=$(dx upload output --brief) dx-jobutil-add-output output &quot;$output&quot; --class=file dx mv ${output} ${prefix}.csv } Create an R script that does the ‘work’ $ vi resources/make_resid.R args&lt;-commandArgs(TRUE) model &lt;- as.formula(args[1]) print(model) pheno = read.csv(&quot;phenofile&quot;,as.is=T) pheno$resid = residuals(lm(model,data=pheno)) pheno$invnt_resid = with(pheno,qnorm((rank(resid,na.last=&quot;keep&quot;)-0.5)/sum(!is.na(resid)))) write.csv(pheno,file=&quot;output&quot;,row.names=F) Build the App $ dx build -f make_residuals --destination=output/YOURNAME/make_residuals Run the App $ dx run output/YOURNAME/make_residuals -iphenofile=phenotype/1KG_pheno.csv \\ -imodel=outcome~sex+Population -iprefix=1kg_pheno_invnt \\ --destination=output/YOURNAME --yes Monitor Progress $ dx watch jobid 9.4.2 Optional Exercise 6) Make QQ plot Make QQ plot of your single variant results. Select results from the multiple chromosome run (chr21 and chr22). You will need to identify the p-value column name. To view the results file try these options: 1) dx download to download the results for viewing. 2) View file through web interface using Visualize ( next to Monitor near top of the page ) and select Gzipped File Previewer 3) Pipe zipped file though regular linux commands dx cat to view column names $ dx cat output/folder/file | gunzip | head Once you know the name of the p-value column, run qqplot first through web interface and then try running interactivly from the web interface then from the command line. $ dx run tools/qqplot Note: the plot label must not contain spaces. 9.4.3 Optional Exercise 7) Run conditional analysis Find the name of one associated variant in the single snp results and rerun the single variant analysis conditioning on that variant (e.g. 22:17105517). Note that the output file name cannot contain a colon (e.g. output file name cannot be single_chr22_single_22:17105517, try single_chr22_single_22_17105517 instead). 9.4.4 Optional Exercise 8) Create a regional association plot using LD extracted from your data set This process requires two steps, one to extract the LD for all variants in the region and one to create the plot. Sequencing data sets often contain variants not in external refernce panels, so it is helpful to create your own LD reference. Step 1: Run GILD (GDS Into LD) App (tools/gild_v1) File inputs: * gds_file -&gt; genotypes/1KG_phase3_subset_chr22.gds Parameter inputs: * lead_snp -&gt; 22:17105517 * start_pos -&gt; 1 * stop_pos -&gt; 51237069 * label for results file -&gt; “LD_chr22” output_LD_filename * output/YOURNAME Note: this can take 10-15 mins to complete Step 2: Run AssocPlot (tools/assocplot) File inputs: datafile -&gt; single variant association results output for chr22 ldfile -&gt; Output file from Step 1 with .ld suffix Parameter inputs (Minimum required to have the App run successfully with GENESIS output): Output folder -&gt; output/YOURNAME Marker Column Name -&gt; snpID P value Column Name -&gt; Score.pval Index SNP -&gt; 22:17105517 "]
]
